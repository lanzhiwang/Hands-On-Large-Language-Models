{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtyN9XsZ245Q"
      },
      "source": [
        "# Tokens and Token Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MQ_9wGWnXBd"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qm1YaZBMni6d"
      },
      "outputs": [],
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "print(\"model:\", model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lYznnYe4KG3o",
        "outputId": "aa6ef0ea-4000-4aad-d499-2b105714fa41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_input: {'input_ids': tensor([[  101,  2478, 19081,  2003,  3733,   999,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "output: torch.Size([1, 7, 768])\n"
          ]
        }
      ],
      "source": [
        "text = \"Using transformers is easy!\"\n",
        "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
        "print(\"encoded_input:\", encoded_input)\n",
        "output = model(**encoded_input)\n",
        "print(\"output:\", output.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(model)\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yw-2SkTSlHkz",
        "outputId": "80aefc6c-2ffe-47cf-a49d-ec5a78a87449"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embeddings.word_embeddings.weight torch.Size([30522, 768])\n",
            "embeddings.position_embeddings.weight torch.Size([512, 768])\n",
            "embeddings.token_type_embeddings.weight torch.Size([2, 768])\n",
            "embeddings.LayerNorm.weight torch.Size([768])\n",
            "embeddings.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.0.output.dense.bias torch.Size([768])\n",
            "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.1.output.dense.bias torch.Size([768])\n",
            "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.2.output.dense.bias torch.Size([768])\n",
            "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.3.output.dense.bias torch.Size([768])\n",
            "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.4.output.dense.bias torch.Size([768])\n",
            "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.5.output.dense.bias torch.Size([768])\n",
            "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.6.output.dense.bias torch.Size([768])\n",
            "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.7.output.dense.bias torch.Size([768])\n",
            "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.8.output.dense.bias torch.Size([768])\n",
            "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.9.output.dense.bias torch.Size([768])\n",
            "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.10.output.dense.bias torch.Size([768])\n",
            "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
            "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
            "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
            "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
            "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
            "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
            "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
            "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
            "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
            "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
            "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
            "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
            "encoder.layer.11.output.dense.bias torch.Size([768])\n",
            "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
            "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
            "pooler.dense.weight torch.Size([768, 768])\n",
            "pooler.dense.bias torch.Size([768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.embeddings.word_embeddings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua_FTS6umFuA",
        "outputId": "4b7acca2-30c0-4e4e-af3c-59e0d3434aef"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30522, 768, padding_idx=0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.embeddings.word_embeddings.weight.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajS79aaulMgR",
        "outputId": "008367ef-89b7-4e5f-ef5d-70a2aa4f2b4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([30522, 768])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "embedding = nn.Embedding.from_pretrained(model.embeddings.word_embeddings.weight)\n",
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-eOtVAK0mcvs",
        "outputId": "d1821325-6d8e-4299-ebe1-461b119bdef1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(30522, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = encoded_input[\"input_ids\"][0]\n",
        "input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pqePgvlnFcj",
        "outputId": "bdd8be8a-d1fa-48d0-f336-93ca3d9e5ba3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  101,  2478, 19081,  2003,  3733,   999,   102])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for id in input_ids:\n",
        "    print(\"id:\", id)\n",
        "    print(tokenizer.decode(id))\n",
        "    print(embedding(id).shape)\n",
        "    print(\"----------\" * 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kpxkyjj3nN0F",
        "outputId": "c696b7b0-246a-4ac5-b1aa-a1cdfc3dca00"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id: tensor(101)\n",
            "[CLS]\n",
            "torch.Size([768])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "id: tensor(2478)\n",
            "using\n",
            "torch.Size([768])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "id: tensor(19081)\n",
            "transformers\n",
            "torch.Size([768])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "id: tensor(2003)\n",
            "is\n",
            "torch.Size([768])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "id: tensor(3733)\n",
            "easy\n",
            "torch.Size([768])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "id: tensor(999)\n",
            "!\n",
            "torch.Size([768])\n",
            "----------------------------------------------------------------------------------------------------\n",
            "id: tensor(102)\n",
            "[SEP]\n",
            "torch.Size([768])\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "id = torch.LongTensor([2478])\n",
        "print(embedding(id).shape)\n",
        "embedding(id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izx8Qc9Ynda5",
        "outputId": "dd780d43-8d9f-44b8-b600-649f35615fbc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 768])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-5.1966e-02,  1.1857e-02, -4.5572e-03, -1.0487e-02, -9.5375e-03,\n",
              "          9.9534e-03, -5.0752e-02,  2.4708e-02,  3.8804e-03, -5.9030e-02,\n",
              "         -1.3676e-02, -2.4394e-02, -2.0969e-03,  4.6606e-02, -5.9757e-02,\n",
              "          1.0715e-02,  8.4250e-04, -1.1981e-03, -2.2384e-02, -6.7643e-02,\n",
              "         -3.3886e-02,  3.2578e-02,  2.9374e-02, -9.1450e-03,  2.3834e-03,\n",
              "         -1.9332e-02, -9.7808e-03, -2.7400e-02, -4.3215e-02, -7.6628e-03,\n",
              "          4.8815e-02,  3.2702e-03,  4.0998e-02, -3.0004e-02,  3.5387e-03,\n",
              "          2.0872e-02, -5.6711e-02,  2.6018e-03, -4.7690e-02, -7.8423e-02,\n",
              "         -1.1707e-03, -1.8451e-02,  1.2292e-02, -5.7301e-02,  1.2055e-03,\n",
              "         -2.4245e-02,  3.3236e-02, -5.9467e-02,  2.2343e-02, -3.4456e-02,\n",
              "         -6.4358e-02, -2.3962e-02,  1.6150e-02, -1.3999e-02, -3.0813e-02,\n",
              "          3.5889e-02, -3.7521e-02, -1.6993e-02,  8.3541e-03,  3.2487e-02,\n",
              "         -2.2775e-02, -3.8754e-03,  2.1007e-02, -3.2956e-02, -2.1765e-02,\n",
              "         -2.4722e-02,  3.9563e-02, -3.5337e-03, -2.2664e-02, -1.5979e-02,\n",
              "          1.1083e-02,  3.9625e-02, -2.6950e-02,  1.6203e-02, -1.3856e-02,\n",
              "         -7.0549e-02,  1.1442e-02, -5.4778e-03, -8.8983e-02, -2.6459e-02,\n",
              "          3.1093e-02, -3.8026e-02, -9.6320e-03,  1.7950e-03, -8.1410e-03,\n",
              "         -4.5082e-03, -5.0036e-02,  1.9631e-02,  4.0710e-02,  1.7695e-02,\n",
              "         -2.3281e-02, -6.0324e-02, -9.4658e-02, -3.5313e-02, -3.7078e-02,\n",
              "          2.7850e-02, -2.3658e-02, -1.7159e-02,  3.7007e-03,  1.9488e-02,\n",
              "         -2.8593e-02, -7.5212e-02,  9.4028e-03,  3.1745e-02, -4.0792e-02,\n",
              "         -8.8721e-04, -2.8914e-02,  9.9273e-03,  3.2650e-02, -4.3174e-02,\n",
              "          1.0523e-02, -2.6545e-02,  8.8060e-03, -5.1379e-02,  3.6105e-02,\n",
              "         -3.2772e-02,  1.4530e-02, -2.6676e-02, -6.4047e-02,  2.1625e-02,\n",
              "         -8.9686e-03, -9.1018e-03,  5.3504e-03, -1.3506e-02, -1.5341e-02,\n",
              "         -3.2927e-03, -1.8370e-02, -5.8709e-02, -1.2824e-02, -9.9110e-02,\n",
              "         -1.8795e-02,  2.4603e-05,  1.3050e-02, -3.8034e-04,  1.6797e-02,\n",
              "         -3.9712e-02, -6.4591e-02,  3.1388e-02, -8.6295e-03,  2.4818e-02,\n",
              "         -3.1899e-02, -3.1777e-02, -1.9471e-03,  5.0476e-02,  3.5737e-03,\n",
              "         -5.0890e-02,  5.0805e-03,  1.6725e-02, -3.2962e-02, -3.0050e-04,\n",
              "          3.9811e-02, -2.5572e-02, -1.0906e-01,  8.9682e-03, -6.3513e-02,\n",
              "         -2.5171e-02, -3.4679e-02,  2.2787e-02,  1.1954e-02, -2.1049e-02,\n",
              "         -6.6721e-03, -1.2512e-02, -4.0729e-02, -3.0820e-02,  8.6365e-03,\n",
              "         -1.3233e-02, -4.0165e-02,  8.5909e-03, -1.4734e-02, -4.0993e-02,\n",
              "          8.9084e-03, -5.4796e-02,  1.6149e-02,  1.4084e-02,  5.9218e-03,\n",
              "         -8.4711e-03, -4.1019e-03, -1.5287e-02,  1.3474e-02,  2.9211e-02,\n",
              "         -2.7676e-02, -4.4520e-03, -6.7123e-02, -2.2049e-02,  1.2062e-02,\n",
              "          3.1230e-03,  7.7375e-03, -5.2860e-02,  1.4134e-02, -1.3196e-02,\n",
              "         -2.7664e-02,  7.2759e-03, -3.2200e-02,  1.1437e-02, -2.5624e-02,\n",
              "          1.3593e-02, -6.7508e-02, -1.0594e-02, -4.5136e-02,  1.3086e-02,\n",
              "         -1.2450e-02, -3.9952e-02, -2.5343e-02, -1.4659e-02, -3.7942e-02,\n",
              "         -6.3617e-02, -1.6788e-02, -4.7756e-02, -6.5201e-02,  4.2236e-03,\n",
              "         -4.1930e-02,  4.9136e-02, -7.8632e-02,  2.0593e-04,  1.8600e-02,\n",
              "         -3.9363e-02, -3.1390e-02,  4.3296e-02,  1.9331e-02, -1.7521e-02,\n",
              "          2.4080e-02, -4.1737e-02,  5.0966e-03, -7.4691e-02, -1.0528e-02,\n",
              "         -9.7547e-03, -9.3404e-02, -1.0833e-02,  1.0175e-02, -3.6493e-02,\n",
              "          2.3538e-02, -3.3888e-02, -6.0927e-02,  4.3555e-02, -2.8422e-02,\n",
              "         -2.3282e-02, -2.4585e-02, -2.6534e-02, -3.4099e-02, -6.4405e-02,\n",
              "         -8.1574e-03, -5.2760e-03, -3.7818e-02, -2.3382e-02,  1.8159e-02,\n",
              "          1.9291e-02, -6.2409e-02, -1.5572e-02, -4.8859e-03, -7.7241e-02,\n",
              "         -1.7396e-02, -6.7394e-04, -4.8480e-02,  2.1926e-02, -1.5953e-02,\n",
              "          3.8684e-02, -3.5521e-02, -1.4818e-02, -3.9068e-02, -2.3360e-02,\n",
              "         -2.8563e-02, -2.3120e-02, -1.2626e-02, -3.3435e-02, -5.3440e-02,\n",
              "         -4.2889e-03, -3.3220e-03, -2.5570e-02,  1.4416e-02, -2.9676e-02,\n",
              "         -3.0296e-03, -4.1684e-02,  9.5566e-03,  3.4979e-03, -4.7462e-02,\n",
              "          9.2700e-03,  6.6181e-04, -1.7462e-02, -6.6660e-02, -7.3951e-02,\n",
              "         -3.9998e-02,  3.5496e-03, -6.0218e-03,  1.3631e-02, -1.0740e-02,\n",
              "          4.4826e-03,  4.8661e-03,  1.6129e-02,  1.2489e-02, -6.5372e-02,\n",
              "         -6.0382e-02,  1.5832e-02, -2.4652e-02, -7.7090e-03, -1.7780e-02,\n",
              "          4.3410e-04,  1.9652e-02, -6.8382e-02,  9.9636e-03, -1.0488e-01,\n",
              "          1.8024e-02,  2.4202e-02,  3.6226e-02, -5.0174e-03, -1.9639e-02,\n",
              "          9.5180e-03, -3.9596e-02, -9.7873e-03, -7.2569e-02, -3.6499e-02,\n",
              "         -3.0158e-02,  3.6456e-02,  2.8073e-02,  2.1440e-02, -2.4768e-02,\n",
              "         -6.5695e-02,  3.6965e-03,  1.2663e-02, -1.4283e-02,  5.1275e-03,\n",
              "         -1.6058e-02, -2.1783e-02, -2.9172e-02,  2.4451e-02, -2.7079e-02,\n",
              "          1.2731e-02,  8.7537e-03, -5.9099e-02, -8.2174e-02, -3.6287e-02,\n",
              "         -3.3733e-02, -5.9184e-03, -2.7261e-02, -4.8040e-02, -1.3110e-02,\n",
              "         -1.7214e-02, -2.6357e-03, -9.7909e-02, -4.5433e-02, -9.2057e-03,\n",
              "          1.2699e-02, -2.1888e-02, -5.1048e-02,  3.2848e-02, -8.9796e-03,\n",
              "         -2.5788e-02,  1.1708e-02, -9.0023e-02, -5.8504e-02, -4.8852e-02,\n",
              "         -6.8367e-02, -1.9024e-02, -5.9710e-02, -7.2567e-03, -9.9661e-03,\n",
              "          7.8408e-04,  2.0415e-02, -2.7583e-02,  1.6443e-02, -5.8962e-02,\n",
              "         -9.6995e-03, -3.6184e-02, -1.8507e-02,  1.4033e-02, -2.7079e-02,\n",
              "          1.0633e-02, -9.6961e-03, -2.4841e-02, -6.5190e-02, -2.8102e-02,\n",
              "         -3.9884e-02, -2.1808e-02, -2.2006e-02, -7.6599e-02, -4.4059e-02,\n",
              "         -4.0487e-02, -1.5682e-02, -3.0701e-02,  3.1318e-02, -4.3977e-02,\n",
              "         -2.1485e-02, -2.4949e-02,  6.1703e-03, -2.6973e-02, -1.3963e-02,\n",
              "         -6.0998e-02, -3.0119e-02,  9.4852e-03,  1.3713e-02, -4.5615e-02,\n",
              "         -7.7330e-02, -7.1105e-02, -1.3504e-02,  7.5799e-04,  4.8264e-03,\n",
              "          1.5514e-02,  2.3083e-03,  1.3619e-02, -3.5542e-02, -1.4027e-02,\n",
              "         -4.8856e-02, -3.6493e-02, -3.7053e-02, -6.3793e-04, -3.3985e-02,\n",
              "         -6.8043e-02, -3.8565e-02, -1.3741e-03, -4.9766e-02, -1.2363e-02,\n",
              "         -1.9684e-02, -1.7740e-02,  4.3095e-02, -6.5704e-02,  1.4690e-02,\n",
              "          1.0448e-02, -8.8735e-02, -1.5968e-02, -3.8796e-02,  4.0221e-02,\n",
              "         -2.0225e-02,  3.4009e-03,  1.1242e-02,  5.6137e-03,  1.5099e-02,\n",
              "         -5.6618e-02, -2.1049e-02,  6.2149e-03, -1.2913e-03, -5.7404e-03,\n",
              "          3.9707e-04, -7.4147e-02, -1.9961e-04, -4.4622e-02, -3.0759e-02,\n",
              "          5.1078e-03, -1.7343e-02, -2.8758e-02, -3.2800e-02, -2.9361e-02,\n",
              "          2.5342e-02,  1.5655e-02, -3.1715e-03,  1.8572e-02, -5.6593e-03,\n",
              "          3.3005e-04,  1.3020e-02, -5.3362e-03, -8.5373e-02, -5.2881e-02,\n",
              "          4.2246e-02,  8.3230e-03, -4.6247e-02, -6.3018e-02, -3.6598e-02,\n",
              "         -3.4432e-02, -7.7871e-03, -2.6900e-02,  1.0922e-02,  1.1161e-02,\n",
              "         -1.4932e-02, -2.1799e-02, -8.4531e-02, -2.6025e-02, -5.4451e-02,\n",
              "         -6.0798e-03, -6.9016e-02, -5.7399e-02,  6.6770e-03,  5.9382e-03,\n",
              "         -2.3483e-02,  8.1416e-03,  9.1529e-03, -5.8566e-02, -8.1817e-04,\n",
              "          2.7261e-02, -7.2364e-03, -4.7228e-03, -4.2536e-02,  9.1081e-03,\n",
              "         -3.7744e-02, -3.6036e-02,  1.3791e-02, -2.7210e-02, -3.1079e-02,\n",
              "          2.6541e-02, -1.4514e-02, -6.3979e-02, -1.1506e-02, -2.0997e-02,\n",
              "         -9.2528e-02, -2.4204e-02, -1.0515e-02, -2.0690e-02, -1.9219e-02,\n",
              "         -5.4832e-02,  5.9967e-03,  1.1259e-02, -2.8663e-02,  1.7748e-02,\n",
              "         -5.8925e-02, -6.9138e-02, -1.0535e-02,  1.0811e-02, -2.2791e-02,\n",
              "          1.8248e-02, -1.2949e-02, -4.1065e-02, -5.5983e-02,  7.7563e-03,\n",
              "         -4.9240e-02, -8.4202e-02,  3.0921e-02, -1.9495e-02, -4.3128e-02,\n",
              "         -2.0996e-02, -4.8174e-02,  9.5631e-03, -3.2260e-02, -1.5189e-02,\n",
              "          1.7769e-02, -7.4581e-02, -2.5067e-02, -2.6538e-02,  2.7444e-02,\n",
              "         -3.8787e-02, -5.5647e-03,  5.0834e-02, -1.2294e-02, -3.6137e-02,\n",
              "          8.5614e-03, -2.4827e-02, -8.7007e-02,  1.6844e-02,  4.3107e-02,\n",
              "         -4.6085e-02, -2.9649e-02,  2.4922e-02, -4.1177e-02, -2.2830e-02,\n",
              "         -5.9245e-03,  3.3673e-03, -1.3225e-03, -4.7746e-02, -1.8881e-02,\n",
              "         -1.0062e-02, -2.6580e-02,  1.0816e-02, -4.3680e-02, -4.9674e-02,\n",
              "         -2.2403e-02,  1.1529e-02, -2.1840e-02,  8.9803e-03, -3.2468e-02,\n",
              "         -4.4070e-02, -8.1987e-03, -3.7283e-02,  1.6395e-02, -4.9245e-02,\n",
              "         -1.0669e-02,  2.4399e-02,  1.9632e-02, -6.7858e-02, -3.8105e-02,\n",
              "         -7.2711e-03,  8.1748e-05, -9.3444e-03, -4.2770e-02, -6.0445e-02,\n",
              "         -1.1093e-02, -1.6101e-02,  1.2524e-02, -7.8805e-03, -2.6322e-02,\n",
              "         -6.4319e-03,  2.0403e-02,  4.8232e-04, -1.9277e-02,  5.9182e-03,\n",
              "          4.8201e-02,  1.5872e-02,  6.5610e-03, -2.9270e-02,  3.1993e-02,\n",
              "          8.2920e-03, -4.4716e-03, -7.2550e-03, -5.5430e-02, -3.8476e-02,\n",
              "         -4.3525e-02, -2.0436e-05,  1.5793e-02, -3.9158e-02, -1.4529e-02,\n",
              "          4.4971e-02, -1.9891e-02, -5.6308e-02, -3.2503e-02,  2.5118e-02,\n",
              "          2.3479e-03, -2.3039e-02,  2.4826e-02,  2.6342e-02, -4.1241e-02,\n",
              "         -6.7506e-02,  7.7316e-03,  5.5582e-03, -5.5863e-02, -3.5583e-02,\n",
              "         -6.4174e-02, -7.4928e-02, -3.8853e-02,  2.6113e-02,  1.5986e-02,\n",
              "         -5.6154e-02, -3.6937e-02, -3.7773e-02, -9.0195e-03, -2.4362e-02,\n",
              "         -3.5036e-02, -5.3449e-02,  1.1561e-03, -3.1129e-02,  2.3495e-02,\n",
              "         -1.0732e-02, -2.5024e-02, -4.3806e-02,  1.3921e-02,  5.5557e-03,\n",
              "         -1.8253e-02, -4.8166e-02, -3.8660e-03,  1.8667e-02, -4.0996e-02,\n",
              "         -1.9922e-02,  1.8518e-02, -1.2852e-02,  5.3787e-02, -5.6505e-02,\n",
              "          4.8075e-02, -3.5248e-02, -1.9933e-02,  3.1600e-02,  4.2172e-02,\n",
              "         -4.4105e-02, -2.3242e-02,  2.0067e-02, -3.4788e-04,  2.2181e-03,\n",
              "         -1.4615e-02, -7.9607e-02, -4.3439e-03, -1.6432e-02, -2.7312e-02,\n",
              "         -1.4619e-02, -9.0795e-02, -2.8027e-02,  1.2587e-02, -4.4869e-03,\n",
              "         -1.5055e-02, -4.4362e-02, -4.6571e-02,  3.4508e-02,  3.5914e-02,\n",
              "         -3.1720e-02, -1.2685e-02, -3.4542e-02,  1.2751e-02, -8.9303e-02,\n",
              "          2.9308e-02, -4.6904e-02,  3.3263e-02, -4.7844e-03, -3.1447e-03,\n",
              "          9.0421e-03, -2.1186e-02, -3.1028e-02, -5.0771e-02,  1.4529e-02,\n",
              "          8.2741e-04, -7.0642e-02, -5.4620e-03, -3.0453e-02,  1.2113e-02,\n",
              "          2.9516e-02, -2.3575e-02, -6.4891e-02,  3.9222e-02,  3.6703e-02,\n",
              "         -8.7663e-02, -1.6892e-02, -6.5801e-02, -1.5593e-02,  1.8430e-02,\n",
              "          1.4659e-02, -2.3028e-02, -3.7453e-02, -3.1807e-02,  5.9801e-02,\n",
              "         -9.6155e-03, -4.6954e-02, -3.5651e-02,  9.1788e-03, -5.2960e-02,\n",
              "          3.1377e-02,  4.3214e-02, -7.9664e-03, -4.3351e-02, -8.0141e-04,\n",
              "         -4.6795e-02, -6.3885e-02, -3.4296e-02, -1.5977e-02,  1.0904e-02,\n",
              "         -3.6676e-02,  5.3138e-04, -2.2036e-02,  4.1063e-03, -2.3969e-02,\n",
              "          1.4974e-02, -2.0515e-02, -5.3582e-02,  1.6440e-02,  8.2763e-03,\n",
              "         -4.7864e-02, -2.2456e-02, -1.8705e-02, -6.0017e-02,  2.6455e-02,\n",
              "          4.7592e-03, -8.8576e-04,  5.3297e-03,  3.6579e-02, -7.0485e-03,\n",
              "         -7.4792e-03, -2.8568e-02,  1.4617e-02, -1.1257e-02, -2.5670e-02,\n",
              "         -5.3210e-02, -4.0056e-02,  1.5634e-02, -9.4421e-02, -3.5558e-02,\n",
              "         -1.0807e-02, -5.2836e-02, -2.9440e-02, -1.1217e-02,  1.4866e-03,\n",
              "          1.6636e-02, -3.1901e-02,  4.3004e-02, -5.0859e-03, -4.7531e-03,\n",
              "         -3.1168e-02, -4.9560e-02, -6.3390e-02, -4.5188e-02, -3.8116e-02,\n",
              "         -1.6715e-02, -2.6795e-02, -4.0865e-03, -2.1262e-02, -1.5740e-02,\n",
              "          1.8545e-02,  1.7876e-02,  1.2891e-02]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vso0OyivoLA2"
      },
      "execution_count": 10,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}